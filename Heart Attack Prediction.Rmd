---
title: "Heart Rate Prediction"
output: html_document
date: "2024-05-31"
---

Ask Phase

Find the combination variables with 70% or more likelhood of having a high chance?

Find the statitic with the highest affect on likelhood chance. 


You will produce a report with the following deliverers: 


1. A clear summary of the business task.

2.A description of all data sources used. 

4. A summary of your analysis 

5. Supporting visualizations and key findings 

6. Your top high-level content recommendations based on your analysis Use the following 


Ask phase Questions.

What is the problem you are trying to solve? 

We want to know the symptoms that result in the having a higher than likely chance of having a heart attack.  

How can your insights drive business decisions?  

Medical professionals could benefit by recommending to patients medical intervention when the measurable statistics. 


Prepare phase questions.

Where is your data stored? 

2 csv files. one wide format style listening the health statistics available. 

How is the data organized?

The data is divided into wide format on daily Activity where each row has lots of different types of informational columns

Are there issues with bias or credibility in this data? 

The data is sufficently large not to introduce bais if the sampling method which is unknown isn't bais. we have to take that on faith since we dont have where it came from listen in the meta data.


Does your data ROCCC? 

We dont know about the reiablity of the data since its not labeld where it came from, we dont know if comes from an orginal source, the data set is not cited, but it current since health statitics are not likely to be out of date.  

How are you addressing licensing, privacy, security, and accessibility?

its cited and public domain data. So the data Roccc. The security of the data is a non issue due it containing no personal information that can be used against the clients thanks to its “depersonalized id system”

How did you verify the data’s integrity?  

CURRENT LOCATION

How does it help you answer your question? 

The data to improve our sales metrics is directly represented in the data.  

The daily activities of these individuals can determine how frequently do our clients use the devices to track 

Their health in order to find out how best to advertise to our clients.

Are there any problems with the data?

The first problem is the stakeholder believes that there are limitations in the data, so we might need to acquire more data to analyze. 

Process 

What tools are you choosing and why? 

I plan on using r due to the large natures of the files in question. 

What steps have you taken to ensure that your data is clean?

Action three: determine the rows is the same with numeric values? 

Yes, All data columns have the same number of columns with numeric values in it.

Action four: inspect each column for null values using filters. 

The data set contains no null values in it. 

Action five: What is the struture of the data?

The data set contains only numeric values. 


● How can you verify that your data is clean and ready to analyze?

● Have you documented your cleaning process so you can review and share those results? 

Key tasks 

1. Check the data for errors. 

2. Choose your tools. We Choose R

3. Transform the data so you can work with it effectively. 

4. Document the cleaning process. Completed my check 


```{r}
library(tidyverse)
library(ggplot2)
library(dplyr)

install.packages("ggplot2")
install.packages("reshape2")
install.packages("corrplot")

library(ggplot2)
library(reshape2)
library(corrplot)

install.packages("skimr")
library(skimr)
```

```{r}
data <- read.csv('heart.csv')

view(data)
```

This is the start of the key task to check the data for errors.
```{r}
skim_without_charts(data)
```
As you can observe the number of n_missing data is zero suggesting all the columns where previously clean for us. 
```{r}
glimpse(data)

```
All the columns have a constant name format therefore do not need to be cleaned. Every column is consitant in under case and there are no spaces between them removing unnecessary underscores. 

Cleaning the data to confirm that all the columns contain numeric values this step proved to be unnecessary, but it was a fun implementation of code

This shows that all the data contains numeric values and not null values suggesting the nulls have been removed from the data prior to our aquiring from kaggle.

checking if all the data contain all the same number of numeric values in each row.

That completes the cleaning process for checking data types, errors and missing or null values.

Now lets inspect the data for outliers we need to use a box and wisker plot using the ggplot methodologies.


```{r}
install.packages("ggplot2")  # Install ggplot2 if you haven't already
library(ggplot2)             # Load the ggplot2 package
```

```{r}
data <- read.csv('heart.csv')


```


```{r}
col_names <- names(data)
print(col_names)
```

```{r}
col_names <- names(data)

plot_boxplot <- function(data, column_name) {
  ggplot(data, aes_string(y = column_name)) +
    geom_boxplot() +
    labs(title = paste("Box Plot of", column_name),
         y = column_name) +
    theme_minimal()
}

# Get column names
col_names <- names(data)

# Loop through column names and create box plots
for (ele in col_names) {
  # Create a box plot for each column
  print(plot_boxplot(data, ele))
}
```

The box plots show many outlines, but they are still in the realm of possible answers for a living person. We can throw out the idea the data contains errors of the having to large of a number variety that would throw off our calculations in our analysis. 

To prepare for the data set, I must find out what is the meaning for each column

Age is how old the person is.

sex is the gender of the person

cp is categorical data that as been label encoded. Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic.

trtpbs - Resting Blood pressing in mm

chol - Cholestoral in mg/dl fetched via BMI sensor

fbs is a boolean variable represented by (fasting blood sugar > 120 mg/dl) ~ 1 = True, 0 = False

restecg - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy

thalachh - Maximum heart rate achieved

oldpeak - Previous peak

slp - Slope

caa - Number of major vessels

thall - Thalium Stress Test result ~ (0,3)

exng - Exercise induced angina ~ 1 = Yes, 0 = No

output - Target variable


Find the combination variables with 70% or more likelhood of having a high chance?

 "age"      "sex"      "cp"       "trtbps"   "chol"     "fbs"      "restecg"  "thalachh" "exng"    
"oldpeak"  "slp"      "caa"      "thall"    "output"  

```{r}
install.packages("dplyr")
library(dplyr)

```
```{r}
num_true <- data %>%
  filter(chol > 250) %>%
  summarize(count = sum(output))

fitler_length <- data %>%
  filter(chol > 250) %>%
  nrow()

print(num_true/fitler_length)

```

```{r}
num_true <- data %>%
  filter(chol > 250 & age > 45 & restecg > 0) %>%
  summarize(count = sum(output))

fitler_length <- data %>%
  filter(chol > 250 & age > 45 & restecg > 0) %>%
  nrow()

print(num_true/fitler_length)

```

```{r}
num_true <- data %>%
  filter(chol > 250 & age > 45 & ) %>%
  summarize(count = sum(output))

fitler_length <- data %>%
  filter(chol > 250 & age > 45 & ) %>%
  nrow()

print(num_true/fitler_length)

```

keep guessing until, I discover a pattern
```{r}
num_true <- data %>%
  filter(chol > 250 & age > 45 & restecg > 0 & trtbps > 150) %>%
  summarize(count = sum(output))

fitler_length <- data %>%
  filter(chol > 250 & age > 45 & restecg > 0 & trtbps > 150) %>%
  nrow()

print(num_true/fitler_length)

```
```{r}
heart_attack_data <- data[data$output == TRUE, ]

hist(heart_attack_data$trtbps, breaks = 10, col = "skyblue", border = "white",
     main = "Distribution of Blood Pressure (trtbps) for Heart Attack Patients",
     xlab = "Resting Blood Pressure (mm Hg)",
     ylab = "Frequency")
Find the statitic with the highest affect on likelhood chance. 

```
Creating a correlcaiton heat map into find the strongest relationship between varaiables. 

```{r}
correlation_matrix <- cor(data)

corrplot(correlation_matrix)
```

Cp has the greatest postive correlation, where old peak has the greatest negative correlation

```{r}
num_true <- data %>%
  filter(cp == 1, oldpeak < 2) %>%
  summarize(count = sum(output))

fitler_length <- data %>%
  filter(cp == 1, oldpeak < 2) %>%
  nrow()

print(num_true/fitler_length)

```
Having chest pain of 1 and old peak less than 2 results in the number of people with a likelyhood of having a heart atttack above 70% meaning we should take preventive action for these individuals. 

```{r}
target_correlations <- correlation_matrix["output", ]

# Convert to absolute values
abs_target_correlations <- abs(target_correlations)

print(abs_target_correlations)
```
exng is the variable with the greatest correlation to the target variable, but thalachh and cp and old peak are also very strong variables to use in predicting heart attacks. 



● How will these insights help answer your business questions?

The goal is to determine actionable health statistics for a person likely to experience a heart attack. With these insights gained from the data analysis we can help medical professionals decision making for heart attack. 


A summary of your analysis:

The two business tasks was to find a set of conditions that had a likelihood greater than 70% and a filtered data set I found old peak less than 2 and chest pain equal to one to have a 82.5% percent chance of having a heart attack. Then I discovered the great correlations to the outcome of having a heart attack is exng is the variable with the greatest correlation to the target variable, but thalachh and cp and old peak are also very strong variables to use in predicting heart attacks. 

● Were you able to answer the business questions? 

Yes the data set contained all the information to analyze the data to solve the business task. 


● What story does your data tell?

The probability of experience a heart attack is found within the data by filtering the data to discover high likelihood of experience a heart attack.  




